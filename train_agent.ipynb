{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anagram Solver RL Training\n",
        "\n",
        "Train LLM agent to solve anagram puzzles using GRPO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install torch transformers datasets accelerate peft trl unsloth matplotlib\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from anagram_game import AnagramSolverEnv\n",
        "from utils import AnagramDataset, create_english_dictionary, correctness_reward_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dictionary and environment\n",
        "create_english_dictionary()\n",
        "env = AnagramSolverEnv()\n",
        "\n",
        "# Generate training data\n",
        "train_tasks = []\n",
        "for difficulty in range(1, 11):\n",
        "    tasks = env.generate(num_of_questions=100, difficulty=difficulty)\n",
        "    train_tasks.extend(tasks)\n",
        "\n",
        "print(f\"Generated {len(train_tasks)} training samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen2.5-1.5B with unsloth\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "print(\"Model loaded with LoRA adapters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_conversation(example):\n",
        "    system_prompt = generate_system_prompt()\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example['question']},\n",
        "        {\"role\": \"assistant\", \"content\": example['answer']}\n",
        "    ]\n",
        "    \n",
        "    formatted = tokenizer.apply_chat_template(\n",
        "        messages, \n",
        "        tokenize=False, \n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    \n",
        "    return {\"text\": formatted}\n",
        "\n",
        "# Convert to HuggingFace dataset\n",
        "train_data = []\n",
        "for item in train_tasks:\n",
        "    train_data.append({\n",
        "        'question': item.question,\n",
        "        'answer': item.answer\n",
        "    })\n",
        "\n",
        "hf_dataset = Dataset.from_list(train_data)\n",
        "formatted_dataset = hf_dataset.map(format_conversation)\n",
        "\n",
        "print(f\"Formatted dataset size: {len(formatted_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Reward Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correctness_reward_func(question: str, response: str) -> float:\n",
        "    \"\"\"Reward function for GRPO training\"\"\"\n",
        "    try:\n",
        "        # Generate dummy task for verification\n",
        "        tasks = env.generate(num_of_questions=1, difficulty=5)\n",
        "        dummy_task = tasks[0]\n",
        "        \n",
        "        # Verify response\n",
        "        result = env.verify(dummy_task, response)\n",
        "        \n",
        "        # Normalize score to [0, 1]\n",
        "        raw_score = result.get('score', 0)\n",
        "        normalized_score = max(0, min(1, (raw_score + 50) / 150))\n",
        "        \n",
        "        return normalized_score\n",
        "        \n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "print(\"Reward function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, num_samples=10):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    correct_count = 0\n",
        "    system_prompt = generate_system_prompt()\n",
        "    \n",
        "    # Generate test tasks\n",
        "    test_tasks = env.generate(num_of_questions=num_samples, difficulty=5)\n",
        "    \n",
        "    for task in test_tasks:\n",
        "        # Prepare input\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": task.question}\n",
        "        ]\n",
        "        \n",
        "        input_text = tokenizer.apply_chat_template(\n",
        "            messages, \n",
        "            tokenize=False, \n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        \n",
        "        # Generate response\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode and evaluate\n",
        "        response = tokenizer.decode(\n",
        "            outputs[0][inputs['input_ids'].shape[1]:], \n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        \n",
        "        result = env.verify(task, response)\n",
        "        if result.get('correct', False):\n",
        "            correct_count += 1\n",
        "    \n",
        "    accuracy = correct_count / num_samples\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate baseline\n",
        "baseline_accuracy = evaluate_model(model, tokenizer)\n",
        "print(f\"Baseline accuracy: {baseline_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRPO Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"./anagram_model\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=50,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    reward_function=correctness_reward_func,\n",
        ")\n",
        "\n",
        "print(\"GRPO Trainer initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting GRPO training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Save model\n",
        "trainer.save_model(\"./anagram_model_final\")\n",
        "print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate trained model\n",
        "trained_accuracy = evaluate_model(model, tokenizer)\n",
        "print(f\"Trained accuracy: {trained_accuracy:.3f}\")\n",
        "\n",
        "# Plot results\n",
        "models = ['Baseline', 'Trained']\n",
        "accuracies = [baseline_accuracy, trained_accuracy]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(models, accuracies, color=['blue', 'green'], alpha=0.7)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "improvement = trained_accuracy - baseline_accuracy\n",
        "print(f\"\\nImprovement: {improvement:.3f} ({improvement/baseline_accuracy:.1%} relative)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on specific examples\n",
        "test_tasks = env.generate(num_of_questions=3, difficulty=7)\n",
        "system_prompt = generate_system_prompt()\n",
        "\n",
        "for i, task in enumerate(test_tasks, 1):\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    print(f\"Anagrams: {task.metadata['anagrams']}\")\n",
        "    print(f\"Correct: {task.metadata['target_words']}\")\n",
        "    \n",
        "    # Generate response\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": task.question}\n",
        "    ]\n",
        "    \n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    result = env.verify(task, response)\n",
        "    \n",
        "    print(f\"Model response: {response}\")\n",
        "    print(f\"Score: {result['score']}, Correct: {result['correct']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}