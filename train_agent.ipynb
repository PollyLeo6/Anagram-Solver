{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anagram Solver RL Training\n",
        "\n",
        "Train LLM agent to solve anagram puzzles using GRPO.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PollyLeo6/Anagram-Solver/blob/main/train_agent.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Quick Setup (Run this first!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-setup for Colab\n",
        "import os\n",
        "\n",
        "if not os.path.exists('anagram_game.py'):\n",
        "    print('ðŸ“¥ Cloning repository...')\n",
        "    !git clone https://github.com/PollyLeo6/Anagram-Solver.git\n",
        "    %cd Anagram-Solver\n",
        "    print('âœ… Repository cloned!')\n",
        "\n",
        "# Generate datasets\n",
        "if not os.path.exists('train_dataset.jsonl'):\n",
        "    print('ðŸ“Š Generating datasets...')\n",
        "    !python utils.py\n",
        "    print('âœ… Datasets generated!')\n",
        "\n",
        "print('ðŸŽ¯ Ready to train!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install torch transformers datasets accelerate peft trl unsloth matplotlib\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from anagram_game import AnagramSolverEnv\n",
        "from utils import create_english_dictionary, correctness_reward_func\n",
        "\n",
        "def generate_system_prompt():\n",
        "    \"\"\"Generate system prompt for anagram solving\"\"\"\n",
        "    return \"\"\"You are an expert anagram solver. Your task is to unscramble letters to form valid English words.\n",
        "\n",
        "Rules:\n",
        "1. Use each letter exactly once\n",
        "2. Form valid English words only\n",
        "3. Respond in JSON format: {\"solutions\": [\"word1\", \"word2\", ...]}\n",
        "4. Order words as they appear in the anagram list\n",
        "\n",
        "Be accurate and follow the format exactly.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen2.5-1.5B with unsloth\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "print(\"Model loaded with LoRA adapters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load existing training data\n",
        "with open('train_dataset.jsonl', 'r') as f:\n",
        "    train_data = []\n",
        "    for line in f:\n",
        "        train_data.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(train_data)} training examples\")\n",
        "\n",
        "# Convert to HuggingFace dataset format\n",
        "def format_conversation(example):\n",
        "    system_prompt = generate_system_prompt()\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example['question']},\n",
        "        {\"role\": \"assistant\", \"content\": example['answer']}\n",
        "    ]\n",
        "    \n",
        "    formatted = tokenizer.apply_chat_template(\n",
        "        messages, \n",
        "        tokenize=False, \n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    \n",
        "    return {\"text\": formatted}\n",
        "\n",
        "hf_dataset = Dataset.from_list(train_data)\n",
        "formatted_dataset = hf_dataset.map(format_conversation)\n",
        "\n",
        "print(f\"Formatted dataset size: {len(formatted_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, num_samples=10):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    env = AnagramSolverEnv()\n",
        "    correct_count = 0\n",
        "    system_prompt = generate_system_prompt()\n",
        "    \n",
        "    # Generate test tasks\n",
        "    test_tasks = env.generate(num_of_questions=num_samples, difficulty=5)\n",
        "    \n",
        "    for task in test_tasks:\n",
        "        # Prepare input\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": task.question}\n",
        "        ]\n",
        "        \n",
        "        input_text = tokenizer.apply_chat_template(\n",
        "            messages, \n",
        "            tokenize=False, \n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        \n",
        "        # Generate response\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode and evaluate\n",
        "        response = tokenizer.decode(\n",
        "            outputs[0][inputs['input_ids'].shape[1]:], \n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        \n",
        "        result = env.verify(task, response)\n",
        "        if result.get('correct', False):\n",
        "            correct_count += 1\n",
        "    \n",
        "    accuracy = correct_count / num_samples\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate baseline\n",
        "baseline_accuracy = evaluate_model(model, tokenizer)\n",
        "print(f\"Baseline accuracy: {baseline_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./anagram_model\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=50,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Save model\n",
        "trainer.save_model(\"./anagram_model_final\")\n",
        "print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate trained model\n",
        "trained_accuracy = evaluate_model(model, tokenizer)\n",
        "print(f\"Trained accuracy: {trained_accuracy:.3f}\")\n",
        "\n",
        "# Plot results\n",
        "models = ['Baseline', 'Trained']\n",
        "accuracies = [baseline_accuracy, trained_accuracy]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(models, accuracies, color=['blue', 'green'], alpha=0.7)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "improvement = trained_accuracy - baseline_accuracy\n",
        "print(f\"\\nImprovement: {improvement:.3f} ({improvement/baseline_accuracy:.1%} relative)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on specific examples\n",
        "env = AnagramSolverEnv()\n",
        "test_tasks = env.generate(num_of_questions=3, difficulty=7)\n",
        "system_prompt = generate_system_prompt()\n",
        "\n",
        "for i, task in enumerate(test_tasks, 1):\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    print(f\"Anagrams: {task.metadata['anagrams']}\")\n",
        "    print(f\"Correct: {task.metadata['target_words']}\")\n",
        "    \n",
        "    # Generate response\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": task.question}\n",
        "    ]\n",
        "    \n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    result = env.verify(task, response)\n",
        "    \n",
        "    print(f\"Model response: {response}\")\n",
        "    print(f\"Score: {result['score']}, Correct: {result['correct']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}