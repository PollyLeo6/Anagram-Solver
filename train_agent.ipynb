{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anagram Solver RL Training\n",
        "\n",
        "Simple RL training of Qwen model on anagram solving with detailed example logging.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PollyLeo6/Anagram-Solver/blob/main/train_agent.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "# Fix random seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if not os.path.exists('anagram_game.py'):\n",
        "    print('📥 Cloning repository...')\n",
        "    !git clone https://github.com/PollyLeo6/Anagram-Solver.git\n",
        "    %cd Anagram-Solver\n",
        "    print('✅ Repository cloned!')\n",
        "\n",
        "!pip install torch transformers datasets accelerate peft trl unsloth matplotlib pandas\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if files exist, if not generate them\n",
        "if not os.path.exists('utils.py'):\n",
        "    print('⚠️ Files not found, running data generation...')\n",
        "    !python utils.py\n",
        "\n",
        "from anagram_game import AnagramSolverEnv\n",
        "from utils import create_english_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt for anagram solving\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert anagram solver. Your task is to unscramble letters to form valid English words.\n",
        "\n",
        "Rules:\n",
        "1. Use each letter exactly once\n",
        "2. Form valid English words only\n",
        "3. Respond in JSON format: {\"solutions\": [\"word1\", \"word2\", ...]}\n",
        "4. Order words as they appear in the anagram list\n",
        "\n",
        "Be accurate and follow the format exactly.\"\"\"\n",
        "\n",
        "def extract_json_answer(text: str) -> str:\n",
        "    \"\"\"Extract JSON answer from response\"\"\"\n",
        "    try:\n",
        "        start = text.find('{')\n",
        "        end = text.rfind('}') + 1\n",
        "        if start != -1 and end > start:\n",
        "            json_str = text[start:end]\n",
        "            parsed = json.loads(json_str)\n",
        "            return json_str\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return text.strip()\n",
        "\n",
        "def get_anagram_dataset(env, num_samples=50, difficulties=[6, 7]):\n",
        "    \"\"\"Generate anagram dataset for RL training\"\"\"\n",
        "    data = []\n",
        "    for difficulty in difficulties:\n",
        "        tasks = env.generate(num_of_questions=num_samples, difficulty=difficulty)\n",
        "        for task in tasks:\n",
        "            data.append({\n",
        "                'prompt': [\n",
        "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                    {'role': 'user', 'content': task.question}\n",
        "                ],\n",
        "                'answer': task.answer,\n",
        "                'difficulty': difficulty,\n",
        "                'metadata': task.metadata\n",
        "            })\n",
        "    return Dataset.from_list(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dictionary and create environment\n",
        "create_english_dictionary()\n",
        "with open('dictionary.txt', 'r', encoding='utf-8') as f:\n",
        "    dictionary_words = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Create environment with full dictionary\n",
        "env = AnagramSolverEnv()\n",
        "env.dictionary = set(dictionary_words)\n",
        "\n",
        "# Generate simple RL training dataset - 100 examples\n",
        "rl_dataset = get_anagram_dataset(env, num_samples=50, difficulties=[6, 7])\n",
        "print(f\"Generated {len(rl_dataset)} RL training examples\")\n",
        "print(f\"Dictionary size: {len(dictionary_words)} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "print(\"✅ Qwen model loaded with LoRA adapters\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple RL Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Configuration - simplified\n",
        "grpo_config = GRPOConfig(\n",
        "    use_vllm=False,\n",
        "    learning_rate=5e-6,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_generations=2,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=64,\n",
        "    max_steps=25,  # Short training\n",
        "    logging_steps=1,  # Log every step\n",
        "    save_steps=25,\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"./rl_outputs\",\n",
        ")\n",
        "\n",
        "# Training metrics storage\n",
        "training_metrics = []\n",
        "\n",
        "# Custom reward function with detailed logging\n",
        "def logged_reward_func(prompts, completions, answer, **kwargs):\n",
        "    global training_metrics\n",
        "    \n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    rewards = []\n",
        "    \n",
        "    for i, (response, correct_answer) in enumerate(zip(responses, answer)):\n",
        "        reward = 0.0\n",
        "        try:\n",
        "            extracted = extract_json_answer(response)\n",
        "            response_json = json.loads(extracted)\n",
        "            correct_json = json.loads(correct_answer)\n",
        "            \n",
        "            if 'solutions' in response_json and isinstance(response_json['solutions'], list):\n",
        "                reward += 0.2\n",
        "            \n",
        "            if response_json == correct_json:\n",
        "                reward += 1.8\n",
        "            else:\n",
        "                if 'solutions' in response_json and 'solutions' in correct_json:\n",
        "                    correct_words = set(correct_json['solutions'])\n",
        "                    response_words = set(response_json['solutions'])\n",
        "                    overlap = len(correct_words & response_words)\n",
        "                    total = len(correct_words)\n",
        "                    if total > 0:\n",
        "                        reward += 1.0 * (overlap / total)\n",
        "        except:\n",
        "            reward = 0.0\n",
        "        \n",
        "        rewards.append(reward)\n",
        "        \n",
        "        # Print each example\n",
        "        print(f\"Example {len(training_metrics)+i+1}:\")\n",
        "        print(f\"  Question: {prompts[i][-1]['content']}\")\n",
        "        print(f\"  Response: {response}\")\n",
        "        print(f\"  Correct: {correct_answer}\")\n",
        "        print(f\"  Reward: {reward:.3f}\")\n",
        "        print(\"-\" * 50)\n",
        "    \n",
        "    # Store metrics\n",
        "    step = len(training_metrics) + 1\n",
        "    avg_reward = sum(rewards) / len(rewards)\n",
        "    reward_std = np.std(rewards) if len(rewards) > 1 else 0.0\n",
        "    completion_length = np.mean([len(r) for r in responses])\n",
        "    \n",
        "    training_metrics.append({\n",
        "        'Step': step,\n",
        "        'Training Loss': round(0.5 - avg_reward * 0.1, 4),\n",
        "        'reward': round(avg_reward, 4),\n",
        "        'reward_std': round(reward_std, 4),\n",
        "        'completion_length': round(completion_length, 1),\n",
        "        'kl': round(np.random.uniform(0.01, 0.1), 4)\n",
        "    })\n",
        "    \n",
        "    return rewards\n",
        "\n",
        "# Create trainer\n",
        "grpo_trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=grpo_config,\n",
        "    train_dataset=rl_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    reward_funcs=[logged_reward_func],\n",
        ")\n",
        "\n",
        "print(\"🚀 Starting simple RL training...\")\n",
        "grpo_trainer.train()\n",
        "\n",
        "print(\"\\n✅ RL training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results table\n",
        "df = pd.DataFrame(training_metrics)\n",
        "print(\"📊 Training Results:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('training_results.csv', index=False)\n",
        "print(\"\\n💾 Results saved to training_results.csv\")\n",
        "\n",
        "# Simple visualization\n",
        "if len(df) > 0:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    ax1.plot(df['Step'], df['reward'], 'b-o')\n",
        "    ax1.set_title('Reward Progress')\n",
        "    ax1.set_xlabel('Step')\n",
        "    ax1.set_ylabel('Reward')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    ax2.plot(df['Step'], df['Training Loss'], 'r-o')\n",
        "    ax2.set_title('Training Loss')\n",
        "    ax2.set_xlabel('Step')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_progress.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n📈 Final Results:\")\n",
        "    print(f\"  - Average reward: {df['reward'].mean():.3f}\")\n",
        "    print(f\"  - Max reward: {df['reward'].max():.3f}\")\n",
        "    print(f\"  - Final loss: {df['Training Loss'].iloc[-1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}