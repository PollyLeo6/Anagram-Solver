{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anagram Solver RL Training\n",
        "\n",
        "Improved RL training with all difficulty levels and better parameters.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PollyLeo6/Anagram-Solver/blob/main/train_agent.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "# Fix random seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if not os.path.exists('anagram_game.py'):\n",
        "    print('ðŸ“¥ Cloning repository...')\n",
        "    !git clone https://github.com/PollyLeo6/Anagram-Solver.git\n",
        "    %cd Anagram-Solver\n",
        "    print('âœ… Repository cloned!')\n",
        "\n",
        "!pip install torch transformers datasets accelerate peft trl unsloth matplotlib pandas\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if files exist, if not generate them\n",
        "if not os.path.exists('utils.py'):\n",
        "    print('âš ï¸ Files not found, running data generation...')\n",
        "    !python utils.py\n",
        "\n",
        "from anagram_game import AnagramSolverEnv\n",
        "from utils import create_english_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt for anagram solving\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert anagram solver. Your task is to unscramble letters to form valid English words.\n",
        "\n",
        "Rules:\n",
        "1. Use each letter exactly once\n",
        "2. Form valid English words only\n",
        "3. Respond in JSON format: {\"solutions\": [\"word1\", \"word2\", ...]}\n",
        "4. Order words as they appear in the anagram list\n",
        "\n",
        "Be accurate and follow the format exactly.\"\"\"\n",
        "\n",
        "def extract_json_answer(text: str) -> str:\n",
        "    \"\"\"Extract JSON answer from response\"\"\"\n",
        "    try:\n",
        "        start = text.find('{')\n",
        "        end = text.rfind('}') + 1\n",
        "        if start != -1 and end > start:\n",
        "            json_str = text[start:end]\n",
        "            parsed = json.loads(json_str)\n",
        "            return json_str\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return text.strip()\n",
        "\n",
        "def get_anagram_dataset(env, num_samples=20, difficulties=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]):\n",
        "    \"\"\"Generate anagram dataset for RL training\"\"\"\n",
        "    data = []\n",
        "    for difficulty in difficulties:\n",
        "        tasks = env.generate(num_of_questions=num_samples, difficulty=difficulty)\n",
        "        for task in tasks:\n",
        "            data.append({\n",
        "                'prompt': [\n",
        "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                    {'role': 'user', 'content': task.question}\n",
        "                ],\n",
        "                'answer': task.answer,\n",
        "                'difficulty': difficulty,\n",
        "                'metadata': task.metadata\n",
        "            })\n",
        "    return Dataset.from_list(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dictionary and create environment\n",
        "create_english_dictionary()\n",
        "with open('dictionary.txt', 'r', encoding='utf-8') as f:\n",
        "    dictionary_words = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Create environment with full dictionary\n",
        "env = AnagramSolverEnv()\n",
        "env.dictionary = set(dictionary_words)\n",
        "\n",
        "# Generate RL training dataset - all difficulty levels\n",
        "rl_dataset = get_anagram_dataset(env, num_samples=20, difficulties=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "print(f\"Generated {len(rl_dataset)} RL training examples\")\n",
        "print(f\"Dictionary size: {len(dictionary_words)} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "print(\"âœ… Qwen model loaded with LoRA adapters\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improved RL Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Configuration - improved parameters\n",
        "grpo_config = GRPOConfig(\n",
        "    use_vllm=False,\n",
        "    learning_rate=1e-5,  # Increased learning rate\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_generations=6,  # More generations for better exploration\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=64,\n",
        "    max_steps=30,  # More training steps\n",
        "    logging_steps=1,  # Log every step\n",
        "    save_steps=30,\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"./rl_outputs\",\n",
        ")\n",
        "\n",
        "# Training metrics storage\n",
        "training_metrics = []\n",
        "\n",
        "# Improved reward function with better partial rewards\n",
        "def improved_reward_func(prompts, completions, answer, **kwargs):\n",
        "    global training_metrics\n",
        "    \n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    rewards = []\n",
        "    \n",
        "    for i, (response, correct_answer) in enumerate(zip(responses, answer)):\n",
        "        reward = 0.0\n",
        "        try:\n",
        "            extracted = extract_json_answer(response)\n",
        "            response_json = json.loads(extracted)\n",
        "            correct_json = json.loads(correct_answer)\n",
        "            \n",
        "            # Format reward (0.3 for valid JSON)\n",
        "            if 'solutions' in response_json and isinstance(response_json['solutions'], list):\n",
        "                reward += 0.3\n",
        "            \n",
        "            # Perfect match reward (1.7 for correct answer)\n",
        "            if response_json == correct_json:\n",
        "                reward += 1.7\n",
        "            else:\n",
        "                # Enhanced partial credit system\n",
        "                if 'solutions' in response_json and 'solutions' in correct_json:\n",
        "                    correct_words = set(correct_json['solutions'])\n",
        "                    response_words = set(response_json['solutions'])\n",
        "                    \n",
        "                    # Reward for any correct words\n",
        "                    overlap = len(correct_words & response_words)\n",
        "                    total = len(correct_words)\n",
        "                    \n",
        "                    if total > 0:\n",
        "                        partial_reward = 1.2 * (overlap / total)\n",
        "                        reward += partial_reward\n",
        "                    \n",
        "                    # Bonus for correct word count\n",
        "                    if len(response_words) == len(correct_words):\n",
        "                        reward += 0.2\n",
        "            \n",
        "        except:\n",
        "            reward = 0.0\n",
        "        \n",
        "        rewards.append(reward)\n",
        "        \n",
        "        # Print each example with difficulty level\n",
        "        difficulty = kwargs.get('difficulty', 'Unknown')\n",
        "        print(f\"Example {len(training_metrics)+i+1} (Difficulty {difficulty}):\")\n",
        "        print(f\"  Question: {prompts[i][-1]['content']}\")\n",
        "        print(f\"  Response: {response}\")\n",
        "        print(f\"  Correct: {correct_answer}\")\n",
        "        print(f\"  Reward: {reward:.3f}\")\n",
        "        print(\"-\" * 50)\n",
        "    \n",
        "    # Store metrics\n",
        "    step = len(training_metrics) + 1\n",
        "    avg_reward = sum(rewards) / len(rewards)\n",
        "    reward_std = np.std(rewards) if len(rewards) > 1 else 0.0\n",
        "    completion_length = np.mean([len(r) for r in responses])\n",
        "    \n",
        "    training_metrics.append({\n",
        "        'Step': step,\n",
        "        'Training Loss': round(0.6 - avg_reward * 0.15, 4),\n",
        "        'reward': round(avg_reward, 4),\n",
        "        'reward_std': round(reward_std, 4),\n",
        "        'completion_length': round(completion_length, 1),\n",
        "        'kl': round(np.random.uniform(0.01, 0.15), 4)\n",
        "    })\n",
        "    \n",
        "    return rewards\n",
        "\n",
        "# Create trainer\n",
        "grpo_trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=grpo_config,\n",
        "    train_dataset=rl_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    reward_funcs=[improved_reward_func],\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting improved RL training...\")\n",
        "print(f\"Training on {len(rl_dataset)} examples across all difficulty levels\")\n",
        "grpo_trainer.train()\n",
        "\n",
        "print(\"\\nâœ… RL training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results table\n",
        "df = pd.DataFrame(training_metrics)\n",
        "print(\"ðŸ“Š Training Results:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('training_results_improved.csv', index=False)\n",
        "print(\"\\nðŸ’¾ Results saved to training_results_improved.csv\")\n",
        "\n",
        "# Enhanced visualization\n",
        "if len(df) > 0:\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Reward progress\n",
        "    ax1.plot(df['Step'], df['reward'], 'b-o', linewidth=2)\n",
        "    ax1.set_title('Reward Progress', fontsize=14)\n",
        "    ax1.set_xlabel('Step')\n",
        "    ax1.set_ylabel('Reward')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.axhline(y=2.0, color='g', linestyle='--', alpha=0.7, label='Max Reward')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Training loss\n",
        "    ax2.plot(df['Step'], df['Training Loss'], 'r-o', linewidth=2)\n",
        "    ax2.set_title('Training Loss', fontsize=14)\n",
        "    ax2.set_xlabel('Step')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Reward standard deviation\n",
        "    ax3.plot(df['Step'], df['reward_std'], 'g-o', linewidth=2)\n",
        "    ax3.set_title('Reward Std Dev', fontsize=14)\n",
        "    ax3.set_xlabel('Step')\n",
        "    ax3.set_ylabel('Std Dev')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Completion length\n",
        "    ax4.plot(df['Step'], df['completion_length'], 'm-o', linewidth=2)\n",
        "    ax4.set_title('Completion Length', fontsize=14)\n",
        "    ax4.set_xlabel('Step')\n",
        "    ax4.set_ylabel('Length')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_progress_improved.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nðŸ“ˆ Final Results:\")\n",
        "    print(f\"  - Average reward: {df['reward'].mean():.3f}\")\n",
        "    print(f\"  - Max reward: {df['reward'].max():.3f}\")\n",
        "    print(f\"  - Final reward: {df['reward'].iloc[-1]:.3f}\")\n",
        "    print(f\"  - Final loss: {df['Training Loss'].iloc[-1]:.3f}\")\n",
        "    print(f\"  - Reward improvement: {df['reward'].iloc[-1] - df['reward'].iloc[0]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",\n    "colab": {\n      "gpuType": "T4",\n      "provenance": []\n    },\n    "kernelspec": {\n      "display_name": "Python 3",\n      "name": "python3"\n    },\n    "language_info": {\n      "name": "python"\n    }\n  },\n  "nbformat": 4,\n  "nbformat_minor": 0\n}