{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anagram Solver RL Training\n",
        "\n",
        "Train LLM agent to solve anagram puzzles using GRPO with train/test split.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PollyLeo6/Anagram-Solver/blob/main/train_agent.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Quick Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Fix random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if not os.path.exists('anagram_game.py'):\n",
        "    print('ðŸ“¥ Cloning repository...')\n",
        "    !git clone https://github.com/PollyLeo6/Anagram-Solver.git\n",
        "    %cd Anagram-Solver\n",
        "    print('âœ… Repository cloned!')\n",
        "\n",
        "print('ðŸŽ¯ Ready to train!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets accelerate peft trl unsloth matplotlib\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer, PPOTrainer, PPOConfig\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from anagram_game import AnagramSolverEnv\n",
        "from utils import create_english_dictionary\n",
        "\n",
        "def generate_system_prompt():\n",
        "    return \"\"\"You are an expert anagram solver. Your task is to unscramble letters to form valid English words.\n",
        "\n",
        "Rules:\n",
        "1. Use each letter exactly once\n",
        "2. Form valid English words only\n",
        "3. Respond in JSON format: {\"solutions\": [\"word1\", \"word2\", ...]}\n",
        "4. Order words as they appear in the anagram list\n",
        "\n",
        "Be accurate and follow the format exactly.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create expanded dictionary\n",
        "full_dictionary = [\n",
        "    # 3-letter words\n",
        "    \"cat\", \"dog\", \"car\", \"sun\", \"run\", \"big\", \"red\", \"hot\", \"old\", \"new\",\n",
        "    \"boy\", \"man\", \"day\", \"way\", \"may\", \"say\", \"try\", \"fly\", \"sky\", \"eye\",\n",
        "    \"bat\", \"hat\", \"mat\", \"sat\", \"fat\", \"pat\", \"rat\", \"eat\", \"tea\", \"sea\",\n",
        "    \n",
        "    # 4-letter words  \n",
        "    \"book\", \"tree\", \"door\", \"fire\", \"wind\", \"star\", \"moon\", \"fish\", \"bird\", \"bear\",\n",
        "    \"house\", \"water\", \"light\", \"dark\", \"fast\", \"slow\", \"good\", \"nice\", \"blue\", \"green\",\n",
        "    \"black\", \"white\", \"small\", \"large\", \"phone\", \"table\", \"chair\", \"paper\", \"money\", \"happy\",\n",
        "    \"love\", \"time\", \"work\", \"play\", \"game\", \"food\", \"hand\", \"head\", \"face\", \"back\",\n",
        "    \n",
        "    # 5-letter words\n",
        "    \"world\", \"right\", \"great\", \"small\", \"every\", \"start\", \"place\", \"where\", \"after\",\n",
        "    \"think\", \"never\", \"again\", \"might\", \"still\", \"while\", \"sound\", \"below\", \"voice\", \"young\",\n",
        "    \"house\", \"point\", \"group\", \"music\", \"party\", \"story\", \"movie\", \"beach\", \"ocean\", \"river\",\n",
        "    \n",
        "    # 6+ letter words for challenge\n",
        "    \"elephant\", \"butterfly\", \"computer\", \"keyboard\", \"monitor\", \"speaker\", \"headset\",\n",
        "    \"programming\", \"algorithm\", \"function\", \"variable\", \"constant\", \"database\",\n",
        "    \"university\", \"education\", \"knowledge\", \"learning\", \"teaching\", \"student\",\n",
        "    \"chocolate\", \"strawberry\", \"blueberry\", \"raspberry\", \"pineapple\", \"watermelon\"\n",
        "]\n",
        "\n",
        "# Split dictionary: 70% train, 30% test\n",
        "random.shuffle(full_dictionary)\n",
        "split_idx = int(0.7 * len(full_dictionary))\n",
        "train_words = full_dictionary[:split_idx]\n",
        "test_words = full_dictionary[split_idx:]\n",
        "\n",
        "print(f\"Total words: {len(full_dictionary)}\")\n",
        "print(f\"Train words: {len(train_words)}\")\n",
        "print(f\"Test words: {len(test_words)}\")\n",
        "print(f\"Train sample: {train_words[:5]}\")\n",
        "print(f\"Test sample: {test_words[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Custom Environment Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainAnagramEnv(AnagramSolverEnv):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dictionary = set(train_words)\n",
        "        \n",
        "class TestAnagramEnv(AnagramSolverEnv):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dictionary = set(test_words)\n",
        "\n",
        "# Create environments\n",
        "train_env = TrainAnagramEnv()\n",
        "test_env = TestAnagramEnv()\n",
        "\n",
        "print(\"âœ… Train and test environments created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "print(\"Model loaded with LoRA adapters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate training data from train_words only\n",
        "train_tasks = []\n",
        "for difficulty in range(1, 8):  # Only up to level 7 for SFT\n",
        "    tasks = train_env.generate(num_of_questions=50, difficulty=difficulty)\n",
        "    train_tasks.extend(tasks)\n",
        "\n",
        "print(f\"Generated {len(train_tasks)} training examples\")\n",
        "\n",
        "# Convert to HuggingFace format\n",
        "def format_conversation(example):\n",
        "    system_prompt = generate_system_prompt()\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example['question']},\n",
        "        {\"role\": \"assistant\", \"content\": example['answer']}\n",
        "    ]\n",
        "    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "train_data = []\n",
        "for task in train_tasks:\n",
        "    train_data.append({\n",
        "        'question': task.question,\n",
        "        'answer': task.answer\n",
        "    })\n",
        "\n",
        "hf_dataset = Dataset.from_list(train_data)\n",
        "formatted_dataset = hf_dataset.map(format_conversation)\n",
        "print(f\"Formatted dataset size: {len(formatted_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, env, difficulty=5, num_samples=20):\n",
        "    \"\"\"Evaluate model on given environment\"\"\"\n",
        "    correct_count = 0\n",
        "    system_prompt = generate_system_prompt()\n",
        "    \n",
        "    test_tasks = env.generate(num_of_questions=num_samples, difficulty=difficulty)\n",
        "    \n",
        "    for task in test_tasks:\n",
        "        try:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": task.question}\n",
        "            ]\n",
        "            \n",
        "            input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "            \n",
        "            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "            result = env.verifier.verify(task, response)\n",
        "            if result:\n",
        "                correct_count += 1\n",
        "        except Exception:\n",
        "            continue\n",
        "    \n",
        "    return correct_count / num_samples\n",
        "\n",
        "# Baseline evaluation\n",
        "print(\"Evaluating baseline...\")\n",
        "baseline_train = evaluate_model(model, tokenizer, train_env, difficulty=5)\n",
        "baseline_test = evaluate_model(model, tokenizer, test_env, difficulty=5)\n",
        "print(f\"Baseline - Train: {baseline_train:.3f}, Test: {baseline_test:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./anagram_sft\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=50,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        ")\n",
        "\n",
        "print(\"Starting SFT training...\")\n",
        "trainer.train()\n",
        "print(\"SFT training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-SFT Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating after SFT...\")\n",
        "sft_train = evaluate_model(model, tokenizer, train_env, difficulty=5)\n",
        "sft_test = evaluate_model(model, tokenizer, test_env, difficulty=5)\n",
        "sft_test_hard = evaluate_model(model, tokenizer, test_env, difficulty=8)\n",
        "\n",
        "print(f\"After SFT - Train: {sft_train:.3f}, Test: {sft_test:.3f}, Test Hard: {sft_test_hard:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RL Training with GRPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reward_function(question: str, response: str) -> float:\n",
        "    \"\"\"Reward function for RL training on test words\"\"\"\n",
        "    try:\n",
        "        # Create a dummy task to verify against\n",
        "        # Extract anagrams from question\n",
        "        import re\n",
        "        anagram_match = re.findall(r\"\\d+\\. (\\w+)\", question)\n",
        "        if not anagram_match:\n",
        "            return 0.0\n",
        "            \n",
        "        # Use test environment for verification\n",
        "        dummy_task = test_env.generate(num_of_questions=1, difficulty=8)[0]\n",
        "        result = test_env.verifier.verify(dummy_task, response)\n",
        "        return 1.0 if result else 0.0\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# Generate RL training data from test words (unseen during SFT)\n",
        "rl_tasks = []\n",
        "for difficulty in [8, 9, 10]:  # Hard levels with test words\n",
        "    tasks = test_env.generate(num_of_questions=30, difficulty=difficulty)\n",
        "    rl_tasks.extend(tasks)\n",
        "\n",
        "print(f\"Generated {len(rl_tasks)} RL training examples from test words\")\n",
        "\n",
        "# Convert to RL format\n",
        "rl_queries = []\n",
        "for task in rl_tasks:\n",
        "    system_prompt = generate_system_prompt()\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": task.question}\n",
        "    ]\n",
        "    query = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    rl_queries.append(query)\n",
        "\n",
        "print(f\"Prepared {len(rl_queries)} RL queries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple RL Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple RL training loop (since GRPO might not be available)\n",
        "from torch.nn import functional as F\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "model.train()\n",
        "\n",
        "print(\"Starting RL training...\")\n",
        "for epoch in range(2):\n",
        "    total_reward = 0\n",
        "    for i, query in enumerate(rl_queries[:50]):  # Limit for demo\n",
        "        # Generate response\n",
        "        inputs = tokenizer(query, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs, \n",
        "                max_new_tokens=50, \n",
        "                temperature=0.8, \n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True\n",
        "            )\n",
        "        \n",
        "        response = tokenizer.decode(outputs.sequences[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "        reward = reward_function(query, response)\n",
        "        total_reward += reward\n",
        "        \n",
        "        # Simple policy gradient update\n",
        "        if reward > 0.5:  # Only update on good responses\n",
        "            model.train()\n",
        "            logits = model(**inputs).logits\n",
        "            loss = -torch.mean(F.log_softmax(logits, dim=-1)) * reward\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    avg_reward = total_reward / min(50, len(rl_queries))\n",
        "    print(f\"Epoch {epoch+1}, Average Reward: {avg_reward:.3f}\")\n",
        "\n",
        "print(\"RL training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Final evaluation...\")\n",
        "final_train = evaluate_model(model, tokenizer, train_env, difficulty=5)\n",
        "final_test = evaluate_model(model, tokenizer, test_env, difficulty=5)\n",
        "final_test_hard = evaluate_model(model, tokenizer, test_env, difficulty=8)\n",
        "\n",
        "print(f\"\\nResults Summary:\")\n",
        "print(f\"Baseline    - Train: {baseline_train:.3f}, Test: {baseline_test:.3f}\")\n",
        "print(f\"After SFT   - Train: {sft_train:.3f}, Test: {sft_test:.3f}, Test Hard: {sft_test_hard:.3f}\")\n",
        "print(f\"After RL    - Train: {final_train:.3f}, Test: {final_test:.3f}, Test Hard: {final_test_hard:.3f}\")\n",
        "\n",
        "# Plot results\n",
        "methods = ['Baseline', 'SFT', 'SFT+RL']\n",
        "train_scores = [baseline_train, sft_train, final_train]\n",
        "test_scores = [baseline_test, sft_test, final_test]\n",
        "test_hard_scores = [0, sft_test_hard, final_test_hard]\n",
        "\n",
        "x = range(len(methods))\n",
        "width = 0.25\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar([i - width for i in x], train_scores, width, label='Train Words', alpha=0.8)\n",
        "plt.bar(x, test_scores, width, label='Test Words (Easy)', alpha=0.8)\n",
        "plt.bar([i + width for i in x], test_hard_scores, width, label='Test Words (Hard)', alpha=0.8)\n",
        "\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Performance: Train vs Test Words')\n",
        "plt.xticks(x, methods)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nKey Insights:\")\n",
        "print(f\"- Overfitting check: Train vs Test gap = {final_train - final_test:.3f}\")\n",
        "print(f\"- RL improvement on unseen hard words: {final_test_hard - sft_test_hard:.3f}\")\n",
        "print(f\"- Generalization: Test performance = {final_test:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on Specific Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on unseen words\n",
        "test_tasks = test_env.generate(num_of_questions=3, difficulty=8)\n",
        "system_prompt = generate_system_prompt()\n",
        "\n",
        "for i, task in enumerate(test_tasks, 1):\n",
        "    print(f\"\\n--- Test Example {i} (Unseen Words) ---\")\n",
        "    print(f\"Anagrams: {task.metadata['anagrams']}\")\n",
        "    print(f\"Correct: {task.metadata['target_words']}\")\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": task.question}\n",
        "    ]\n",
        "    \n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    result = test_env.verifier.verify(task, response)\n",
        "    \n",
        "    print(f\"Model response: {response}\")\n",
        "    print(f\"Correct: {result}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}