{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anagram Solver RL Training\n",
        "\n",
        "Pure RL training of Qwen model on anagram solving with detailed metrics tracking.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PollyLeo6/Anagram-Solver/blob/main/train_agent.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "# Fix random seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if not os.path.exists('anagram_game.py'):\n",
        "    print('📥 Cloning repository...')\n",
        "    !git clone https://github.com/PollyLeo6/Anagram-Solver.git\n",
        "    %cd Anagram-Solver\n",
        "    print('✅ Repository cloned!')\n",
        "\n",
        "!pip install torch transformers datasets accelerate peft trl unsloth matplotlib pandas\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if files exist, if not generate them\n",
        "if not os.path.exists('utils.py'):\n",
        "    print('⚠️ Files not found, running data generation...')\n",
        "    !python utils.py\n",
        "\n",
        "from anagram_game import AnagramSolverEnv\n",
        "from utils import create_english_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt for anagram solving\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert anagram solver. Your task is to unscramble letters to form valid English words.\n",
        "\n",
        "Rules:\n",
        "1. Use each letter exactly once\n",
        "2. Form valid English words only\n",
        "3. Respond in JSON format: {\"solutions\": [\"word1\", \"word2\", ...]}\n",
        "4. Order words as they appear in the anagram list\n",
        "\n",
        "Be accurate and follow the format exactly.\"\"\"\n",
        "\n",
        "def extract_json_answer(text: str) -> str:\n",
        "    \"\"\"Extract JSON answer from response\"\"\"\n",
        "    try:\n",
        "        start = text.find('{')\n",
        "        end = text.rfind('}') + 1\n",
        "        if start != -1 and end > start:\n",
        "            json_str = text[start:end]\n",
        "            parsed = json.loads(json_str)\n",
        "            return json_str\n",
        "        return text.strip()\n",
        "    except:\n",
        "        return text.strip()\n",
        "\n",
        "def get_anagram_dataset(env, num_samples=100, difficulties=[5, 6, 7, 8]):\n",
        "    \"\"\"Generate anagram dataset for RL training\"\"\"\n",
        "    data = []\n",
        "    for difficulty in difficulties:\n",
        "        tasks = env.generate(num_of_questions=num_samples, difficulty=difficulty)\n",
        "        for task in tasks:\n",
        "            data.append({\n",
        "                'prompt': [\n",
        "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                    {'role': 'user', 'content': task.question}\n",
        "                ],\n",
        "                'answer': task.answer,\n",
        "                'difficulty': difficulty,\n",
        "                'metadata': task.metadata\n",
        "            })\n",
        "    return Dataset.from_list(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dictionary and create environment\n",
        "create_english_dictionary()\n",
        "with open('dictionary.txt', 'r', encoding='utf-8') as f:\n",
        "    dictionary_words = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Create environment with full dictionary\n",
        "env = AnagramSolverEnv()\n",
        "env.dictionary = set(dictionary_words)\n",
        "\n",
        "# Generate RL training dataset\n",
        "rl_dataset = get_anagram_dataset(env, num_samples=100, difficulties=[5, 6, 7, 8])\n",
        "print(f\"Generated {len(rl_dataset)} RL training examples\")\n",
        "print(f\"Dictionary size: {len(dictionary_words)} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reward Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def anagram_reward_func(prompts, completions, answer, **kwargs) -> List[float]:\n",
        "    \"\"\"Reward function for anagram solving with detailed logging\"\"\"\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    rewards = []\n",
        "    \n",
        "    for i, (response, correct_answer) in enumerate(zip(responses, answer)):\n",
        "        reward = 0.0\n",
        "        \n",
        "        try:\n",
        "            # Extract JSON from response\n",
        "            extracted = extract_json_answer(response)\n",
        "            response_json = json.loads(extracted)\n",
        "            correct_json = json.loads(correct_answer)\n",
        "            \n",
        "            # Format reward (0.2 for valid JSON)\n",
        "            if 'solutions' in response_json and isinstance(response_json['solutions'], list):\n",
        "                reward += 0.2\n",
        "            \n",
        "            # Correctness reward (1.8 for correct answer)\n",
        "            if response_json == correct_json:\n",
        "                reward += 1.8\n",
        "            else:\n",
        "                # Partial credit for correct words\n",
        "                if 'solutions' in response_json and 'solutions' in correct_json:\n",
        "                    correct_words = set(correct_json['solutions'])\n",
        "                    response_words = set(response_json['solutions'])\n",
        "                    overlap = len(correct_words & response_words)\n",
        "                    total = len(correct_words)\n",
        "                    if total > 0:\n",
        "                        reward += 1.0 * (overlap / total)\n",
        "            \n",
        "        except Exception as e:\n",
        "            # No reward for invalid responses\n",
        "            reward = 0.0\n",
        "        \n",
        "        rewards.append(reward)\n",
        "    \n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "\n",
        "print(\"✅ Qwen model loaded with LoRA adapters\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRPO Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRPO Configuration for detailed tracking\n",
        "grpo_config = GRPOConfig(\n",
        "    use_vllm=False,  # Disable for better logging\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=1,  # Log every step\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_generations=4,\n",
        "    max_prompt_length=512,\n",
        "    max_completion_length=128,\n",
        "    max_steps=500,  # More steps for detailed tracking\n",
        "    save_steps=100,\n",
        "    eval_steps=50,\n",
        "    max_grad_norm=1.0,\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"./rl_outputs\",\n",
        "    logging_dir=\"./rl_logs\",\n",
        "    dataloader_num_workers=0,\n",
        ")\n",
        "\n",
        "print(\"GRPO Configuration:\")\n",
        "print(f\"- Learning rate: {grpo_config.learning_rate}\")\n",
        "print(f\"- Batch size: {grpo_config.per_device_train_batch_size}\")\n",
        "print(f\"- Max steps: {grpo_config.max_steps}\")\n",
        "print(f\"- Num generations: {grpo_config.num_generations}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RL Training with Metrics Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom callback for detailed metrics tracking\n",
        "class MetricsCallback:\n",
        "    def __init__(self):\n",
        "        self.metrics_history = []\n",
        "    \n",
        "    def on_log(self, logs):\n",
        "        if logs:\n",
        "            self.metrics_history.append(logs)\n",
        "    \n",
        "    def get_dataframe(self):\n",
        "        if not self.metrics_history:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        df = pd.DataFrame(self.metrics_history)\n",
        "        # Ensure required columns exist\n",
        "        required_cols = ['step', 'train_loss', 'reward', 'reward_std', 'completion_length', 'kl']\n",
        "        for col in required_cols:\n",
        "            if col not in df.columns:\n",
        "                df[col] = 0.0\n",
        "        return df[required_cols]\n",
        "\n",
        "# Initialize callback\n",
        "metrics_callback = MetricsCallback()\n",
        "\n",
        "# Create GRPO trainer\n",
        "grpo_trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=grpo_config,\n",
        "    train_dataset=rl_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    reward_function=anagram_reward_func,\n",
        ")\n",
        "\n",
        "# Add callback\n",
        "grpo_trainer.add_callback(metrics_callback)\n",
        "\n",
        "print(\"🚀 Starting RL training...\")\n",
        "print(f\"Training on {len(rl_dataset)} examples\")\n",
        "\n",
        "# Start training\n",
        "grpo_trainer.train()\n",
        "\n",
        "print(\"✅ RL training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Metrics Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get training metrics\n",
        "metrics_df = metrics_callback.get_dataframe()\n",
        "\n",
        "if not metrics_df.empty:\n",
        "    print(\"📊 Training Metrics Summary:\")\n",
        "    print(metrics_df.describe())\n",
        "    \n",
        "    # Display first 10 and last 10 rows\n",
        "    print(\"\\n📈 First 10 training steps:\")\n",
        "    print(metrics_df.head(10).to_string(index=False))\n",
        "    \n",
        "    print(\"\\n📈 Last 10 training steps:\")\n",
        "    print(metrics_df.tail(10).to_string(index=False))\n",
        "    \n",
        "    # Save to CSV\n",
        "    metrics_df.to_csv('training_metrics.csv', index=False)\n",
        "    print(\"\\n💾 Metrics saved to training_metrics.csv\")\n",
        "else:\n",
        "    print(\"⚠️ No metrics collected during training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Progress Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not metrics_df.empty:\n",
        "    # Create subplots for different metrics\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('RL Training Progress', fontsize=16)\n",
        "    \n",
        "    # Training Loss\n",
        "    axes[0, 0].plot(metrics_df['step'], metrics_df['train_loss'])\n",
        "    axes[0, 0].set_title('Training Loss')\n",
        "    axes[0, 0].set_xlabel('Step')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Reward\n",
        "    axes[0, 1].plot(metrics_df['step'], metrics_df['reward'])\n",
        "    axes[0, 1].set_title('Reward')\n",
        "    axes[0, 1].set_xlabel('Step')\n",
        "    axes[0, 1].set_ylabel('Reward')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Reward Standard Deviation\n",
        "    axes[0, 2].plot(metrics_df['step'], metrics_df['reward_std'])\n",
        "    axes[0, 2].set_title('Reward Standard Deviation')\n",
        "    axes[0, 2].set_xlabel('Step')\n",
        "    axes[0, 2].set_ylabel('Reward Std')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Completion Length\n",
        "    axes[1, 0].plot(metrics_df['step'], metrics_df['completion_length'])\n",
        "    axes[1, 0].set_title('Completion Length')\n",
        "    axes[1, 0].set_xlabel('Step')\n",
        "    axes[1, 0].set_ylabel('Length')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # KL Divergence\n",
        "    axes[1, 1].plot(metrics_df['step'], metrics_df['kl'])\n",
        "    axes[1, 1].set_title('KL Divergence')\n",
        "    axes[1, 1].set_xlabel('Step')\n",
        "    axes[1, 1].set_ylabel('KL')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Combined reward and loss\n",
        "    ax2 = axes[1, 2]\n",
        "    ax2.plot(metrics_df['step'], metrics_df['reward'], 'g-', label='Reward')\n",
        "    ax2.set_xlabel('Step')\n",
        "    ax2.set_ylabel('Reward', color='g')\n",
        "    ax2.tick_params(axis='y', labelcolor='g')\n",
        "    \n",
        "    ax3 = ax2.twinx()\n",
        "    ax3.plot(metrics_df['step'], metrics_df['train_loss'], 'r-', label='Loss')\n",
        "    ax3.set_ylabel('Loss', color='r')\n",
        "    ax3.tick_params(axis='y', labelcolor='r')\n",
        "    \n",
        "    axes[1, 2].set_title('Reward vs Loss')\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_progress.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"📊 Training progress plots saved as 'training_progress.png'\")\n",
        "else:\n",
        "    print(\"⚠️ No metrics to visualize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Inference Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model_inference(model, tokenizer, env, num_tests=10):\n",
        "    \"\"\"Test trained model on new anagram examples\"\"\"\n",
        "    print(\"🧪 Testing model inference...\")\n",
        "    \n",
        "    # Generate test examples\n",
        "    test_tasks = env.generate(num_of_questions=num_tests, difficulty=6)\n",
        "    \n",
        "    results = []\n",
        "    correct_count = 0\n",
        "    \n",
        "    for i, task in enumerate(test_tasks, 1):\n",
        "        print(f\"\\n--- Test {i}/{num_tests} ---\")\n",
        "        print(f\"Question: {task.question}\")\n",
        "        print(f\"Correct answer: {task.answer}\")\n",
        "        \n",
        "        try:\n",
        "            # Prepare input\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": task.question}\n",
        "            ]\n",
        "            \n",
        "            input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "            \n",
        "            # Generate response\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=100,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "            print(f\"Model response: {response}\")\n",
        "            \n",
        "            # Verify correctness\n",
        "            is_correct = env.verifier.verify(task, response)\n",
        "            print(f\"Correct: {'✅' if is_correct else '❌'}\")\n",
        "            \n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            \n",
        "            results.append({\n",
        "                'question': task.question,\n",
        "                'correct_answer': task.answer,\n",
        "                'model_response': response,\n",
        "                'is_correct': is_correct,\n",
        "                'difficulty': task.difficulty\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            results.append({\n",
        "                'question': task.question,\n",
        "                'correct_answer': task.answer,\n",
        "                'model_response': f\"Error: {e}\",\n",
        "                'is_correct': False,\n",
        "                'difficulty': task.difficulty\n",
        "            })\n",
        "    \n",
        "    accuracy = correct_count / num_tests\n",
        "    print(f\"\\n📊 Inference Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.2%} ({correct_count}/{num_tests})\")\n",
        "    \n",
        "    # Save results\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv('inference_results.csv', index=False)\n",
        "    print(f\"💾 Results saved to 'inference_results.csv'\")\n",
        "    \n",
        "    return accuracy, results\n",
        "\n",
        "# Run inference tests\n",
        "accuracy, test_results = test_model_inference(model, tokenizer, env, num_tests=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🎯 RL Training Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not metrics_df.empty:\n",
        "    print(f\"📈 Training Progress:\")\n",
        "    print(f\"  - Total steps: {len(metrics_df)}\")\n",
        "    print(f\"  - Final reward: {metrics_df['reward'].iloc[-1]:.4f}\")\n",
        "    print(f\"  - Final loss: {metrics_df['train_loss'].iloc[-1]:.4f}\")\n",
        "    print(f\"  - Average reward: {metrics_df['reward'].mean():.4f}\")\n",
        "    print(f\"  - Max reward: {metrics_df['reward'].max():.4f}\")\n",
        "\n",
        "print(f\"\\n🧪 Inference Performance:\")\n",
        "print(f\"  - Test accuracy: {accuracy:.2%}\")\n",
        "print(f\"  - Model: Qwen2.5-1.5B-Instruct + LoRA\")\n",
        "print(f\"  - Training method: Pure RL (GRPO)\")\n",
        "\n",
        "print(f\"\\n📁 Generated Files:\")\n",
        "print(f\"  - training_metrics.csv: Detailed training metrics\")\n",
        "print(f\"  - training_progress.png: Training visualization\")\n",
        "print(f\"  - inference_results.csv: Test results\")\n",
        "print(f\"  - Model saved in: ./rl_outputs/\")\n",
        "\n",
        "print(\"\\n✅ Training and evaluation complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}